{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE: https://www.kaggle.com/code/sercanyesiloz/pyspark-tutorial/notebook\n",
    "import os\n",
    "import warnings\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import split, count, when, isnan, col, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acorn_details.csv                     informations_households.csv\n",
      "\u001b[34mdaily_dataset\u001b[m\u001b[m                         uk_bank_holidays.csv\n",
      "darksky_parameters_documentation.html weather_daily_darksky.csv\n",
      "\u001b[34mhalfhourly_dataset\u001b[m\u001b[m                    weather_hourly_darksky.csv\n",
      "\u001b[34mhhblock_dataset\u001b[m\u001b[m\n",
      "\n",
      "./darksky/daily_dataset:\n",
      "\u001b[34mdaily_dataset\u001b[m\u001b[m\n",
      "\n",
      "./darksky/daily_dataset/daily_dataset:\n",
      "block_111.csv\n",
      "\n",
      "./darksky/halfhourly_dataset:\n",
      "\u001b[34mhalfhourly_dataset\u001b[m\u001b[m\n",
      "\n",
      "./darksky/halfhourly_dataset/halfhourly_dataset:\n",
      "block_102.csv block_22.csv  block_42.csv  block_62.csv  block_82.csv\n",
      "block_103.csv block_23.csv  block_43.csv  block_63.csv  block_83.csv\n",
      "block_104.csv block_24.csv  block_44.csv  block_64.csv  block_84.csv\n",
      "block_105.csv block_25.csv  block_45.csv  block_65.csv  block_85.csv\n",
      "block_106.csv block_26.csv  block_46.csv  block_66.csv  block_86.csv\n",
      "block_107.csv block_27.csv  block_47.csv  block_67.csv  block_87.csv\n",
      "block_108.csv block_28.csv  block_48.csv  block_68.csv  block_88.csv\n",
      "block_109.csv block_29.csv  block_49.csv  block_69.csv  block_89.csv\n",
      "block_11.csv  block_3.csv   block_5.csv   block_7.csv   block_9.csv\n",
      "block_110.csv block_30.csv  block_50.csv  block_70.csv  block_90.csv\n",
      "block_111.csv block_31.csv  block_51.csv  block_71.csv  block_91.csv\n",
      "block_12.csv  block_32.csv  block_52.csv  block_72.csv  block_92.csv\n",
      "block_13.csv  block_33.csv  block_53.csv  block_73.csv  block_93.csv\n",
      "block_14.csv  block_34.csv  block_54.csv  block_74.csv  block_94.csv\n",
      "block_15.csv  block_35.csv  block_55.csv  block_75.csv  block_95.csv\n",
      "block_16.csv  block_36.csv  block_56.csv  block_76.csv  block_96.csv\n",
      "block_17.csv  block_37.csv  block_57.csv  block_77.csv  block_97.csv\n",
      "block_18.csv  block_38.csv  block_58.csv  block_78.csv  block_98.csv\n",
      "block_19.csv  block_39.csv  block_59.csv  block_79.csv  block_99.csv\n",
      "block_2.csv   block_4.csv   block_6.csv   block_8.csv\n",
      "block_20.csv  block_40.csv  block_60.csv  block_80.csv\n",
      "block_21.csv  block_41.csv  block_61.csv  block_81.csv\n",
      "\n",
      "./darksky/hhblock_dataset:\n",
      "\u001b[34mhhblock_dataset\u001b[m\u001b[m\n",
      "\n",
      "./darksky/hhblock_dataset/hhblock_dataset:\n",
      "block_0.csv   block_19.csv  block_4.csv   block_60.csv  block_81.csv\n",
      "block_1.csv   block_2.csv   block_40.csv  block_61.csv  block_82.csv\n",
      "block_10.csv  block_20.csv  block_41.csv  block_62.csv  block_83.csv\n",
      "block_100.csv block_21.csv  block_42.csv  block_63.csv  block_84.csv\n",
      "block_101.csv block_22.csv  block_43.csv  block_64.csv  block_85.csv\n",
      "block_102.csv block_23.csv  block_44.csv  block_65.csv  block_86.csv\n",
      "block_103.csv block_24.csv  block_45.csv  block_66.csv  block_87.csv\n",
      "block_104.csv block_25.csv  block_46.csv  block_67.csv  block_88.csv\n",
      "block_105.csv block_26.csv  block_47.csv  block_68.csv  block_89.csv\n",
      "block_106.csv block_27.csv  block_48.csv  block_69.csv  block_9.csv\n",
      "block_107.csv block_28.csv  block_49.csv  block_7.csv   block_90.csv\n",
      "block_108.csv block_29.csv  block_5.csv   block_70.csv  block_91.csv\n",
      "block_109.csv block_3.csv   block_50.csv  block_71.csv  block_92.csv\n",
      "block_11.csv  block_30.csv  block_51.csv  block_72.csv  block_93.csv\n",
      "block_110.csv block_31.csv  block_52.csv  block_73.csv  block_94.csv\n",
      "block_111.csv block_32.csv  block_53.csv  block_74.csv  block_95.csv\n",
      "block_12.csv  block_33.csv  block_54.csv  block_75.csv  block_96.csv\n",
      "block_13.csv  block_34.csv  block_55.csv  block_76.csv  block_97.csv\n",
      "block_14.csv  block_35.csv  block_56.csv  block_77.csv  block_98.csv\n",
      "block_15.csv  block_36.csv  block_57.csv  block_78.csv  block_99.csv\n",
      "block_16.csv  block_37.csv  block_58.csv  block_79.csv\n",
      "block_17.csv  block_38.csv  block_59.csv  block_8.csv\n",
      "block_18.csv  block_39.csv  block_6.csv   block_80.csv\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import zipfile\n",
    "from os.path import exists\n",
    "\n",
    "# Authenticate\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# check for file and if it does not exist, download and extract dataset\n",
    "if os.path.exists('./darksky') is False:\n",
    "    api.dataset_download_files('jeanmidev/smart-meters-in-london', path = './')\n",
    "    with zipfile.ZipFile('./smart-meters-in-london.zip', 'r') as zipref:\n",
    "        zipref.extractall('./darksky')\n",
    "\n",
    "# list files\n",
    "!ls -R ./darksky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/20 19:15:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://harddritoosmall:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TeamDarkSky</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1147d44d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary modules\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Initialize a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('TeamDarkSky') \\\n",
    "    .config(\"spark.jars\", \"/Library/Frameworks/Python.framework/Versions/3.11/bin/sqljdbc_12.2/enu/mssql-jdbc-12.2.0.jre8.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caling the session variable object\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- day: timestamp (nullable = true)\n",
      " |-- energy_median: double (nullable = true)\n",
      " |-- energy_mean: double (nullable = true)\n",
      " |-- energy_max: double (nullable = true)\n",
      " |-- energy_count: integer (nullable = true)\n",
      " |-- energy_std: double (nullable = true)\n",
      " |-- energy_sum: double (nullable = true)\n",
      " |-- energy_min: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create Spark dataframe for daily energy usage data\n",
    "\n",
    "dailyDf = spark.read.csv('./darksky/daily_dataset/daily_dataset', inferSchema=True, header=True)\n",
    "\n",
    "dailyDf.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|MAIN CATEGORIES|      CATEGORIES|           REFERENCE|ACORN-A|ACORN-B|ACORN-C|ACORN-D|ACORN-E|ACORN-F|ACORN-G|ACORN-H|ACORN-I|ACORN-J|ACORN-K|ACORN-L|ACORN-M|ACORN-N|ACORN-O|ACORN-P|ACORN-Q|\n",
      "+---------------+----------------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|     POPULATION|             Age|             Age 0-4|   77.0|   83.0|   72.0|  100.0|  120.0|   77.0|   97.0|   97.0|   63.0|  119.0|   67.0|  114.0|  113.0|   89.0|  123.0|  138.0|  133.0|\n",
      "|     POPULATION|             Age|            Age 5-17|  117.0|  109.0|   87.0|   69.0|   94.0|   95.0|  102.0|  106.0|   67.0|   95.0|   64.0|  108.0|  116.0|   86.0|   89.0|  136.0|  106.0|\n",
      "|     POPULATION|             Age|           Age 18-24|   64.0|   73.0|   67.0|  107.0|  100.0|   71.0|   83.0|   89.0|   62.0|  104.0|  459.0|   97.0|   96.0|   86.0|  117.0|  109.0|  110.0|\n",
      "|     POPULATION|             Age|           Age 25-34|   52.0|   63.0|   62.0|  197.0|  151.0|   66.0|   90.0|   88.0|   63.0|  132.0|  145.0|  109.0|   96.0|   90.0|  140.0|  120.0|  120.0|\n",
      "|     POPULATION|             Age|           Age 35-49|  102.0|  105.0|   91.0|  124.0|  118.0|   93.0|  102.0|  103.0|   76.0|  111.0|   67.0|   99.0|   98.0|   90.0|  102.0|  103.0|  100.0|\n",
      "|     POPULATION|             Age|           Age 50-64|  124.0|  121.0|  120.0|   72.0|   82.0|  126.0|  109.0|  107.0|  112.0|   90.0|   41.0|   95.0|   96.0|  103.0|   89.0|   78.0|   89.0|\n",
      "|     POPULATION|             Age|          Aged 65-74|  125.0|  120.0|  152.0|   55.0|   61.0|  144.0|  108.0|  104.0|  182.0|   72.0|   29.0|   91.0|   93.0|  125.0|   73.0|   59.0|   76.0|\n",
      "|     POPULATION|             Age|        Aged 75 plus|  112.0|  103.0|  157.0|   49.0|   57.0|  117.0|   98.0|   96.0|  220.0|   66.0|   32.0|   87.0|   96.0|  152.0|   72.0|   56.0|   76.0|\n",
      "|     POPULATION|       Geography|             England|  107.0|  101.0|  103.0|  114.0|  106.0|   75.0|  107.0|  106.0|  102.0|  106.0|   95.0|   93.0|   97.0|   89.0|   97.0|  110.0|   97.0|\n",
      "|     POPULATION|       Geography|    Northern Ireland|   30.0|   95.0|   45.0|    2.0|   49.0|  462.0|   53.0|  104.0|   30.0|   91.0|   56.0|   87.0|  131.0|   67.0|   95.0|   75.0|   43.0|\n",
      "|     POPULATION|       Geography|            Scotland|   93.0|  105.0|   87.0|   47.0|   93.0|  144.0|   54.0|   46.0|   97.0|   53.0|  167.0|  114.0|  121.0|  194.0|  139.0|   31.0|  183.0|\n",
      "|     POPULATION|       Geography|               Wales|   22.0|   73.0|   99.0|   10.0|   46.0|  249.0|   77.0|   84.0|  113.0|   73.0|   98.0|  211.0|  104.0|  150.0|   88.0|   54.0|   45.0|\n",
      "|     POPULATION|       Ethnicity|               White|   98.0|  106.0|  110.0|   83.0|   93.0|  114.0|   94.0|  102.0|  113.0|  103.0|   81.0|   92.0|  106.0|  111.0|  100.0|   80.0|   98.0|\n",
      "|     POPULATION|       Ethnicity|               Mixed|  117.0|   79.0|   55.0|  248.0|  149.0|   27.0|   84.0|   90.0|   38.0|  116.0|  175.0|   73.0|   88.0|   48.0|  104.0|  193.0|  137.0|\n",
      "|     POPULATION|       Ethnicity|               Asian|  107.0|   67.0|   34.0|  102.0|  116.0|    8.0|  212.0|   89.0|   21.0|   71.0|  159.0|  276.0|   55.0|   24.0|  102.0|  154.0|   79.0|\n",
      "|     POPULATION|       Ethnicity|               Black|   48.0|   34.0|   21.0|  215.0|  158.0|    6.0|   71.0|   87.0|   14.0|   76.0|  213.0|   71.0|   69.0|   32.0|   85.0|  389.0|  174.0|\n",
      "|     POPULATION|       Ethnicity|     Other ethnicity|  174.0|   84.0|   47.0|  304.0|  168.0|   18.0|  105.0|   76.0|   30.0|   81.0|  327.0|   86.0|   52.0|   34.0|   99.0|  179.0|   94.0|\n",
      "|     POPULATION|Country of Birth|            UK / ROI|   95.0|  105.0|  107.0|   75.0|   91.0|  110.0|   98.0|  103.0|  109.0|  100.0|   79.0|   98.0|  105.0|  109.0|   98.0|   88.0|   99.0|\n",
      "|     POPULATION|Country of Birth|EU: Member countr...|  202.0|   85.0|   68.0|  471.0|  166.0|   50.0|   79.0|   70.0|   50.0|  103.0|  233.0|   64.0|   63.0|   46.0|   87.0|  137.0|   75.0|\n",
      "|     POPULATION|Country of Birth|EU: Accession cou...|   55.0|   46.0|   39.0|  184.0|  183.0|   34.0|   90.0|   82.0|   37.0|  130.0|  260.0|   99.0|   81.0|   52.0|  225.0|  155.0|  135.0|\n",
      "+---------------+----------------+--------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# acorn_detals.csv\n",
    "\n",
    "acorn = spark.read.csv('./darksky/acorn_details.csv', inferSchema=True, header=True)\n",
    "\n",
    "acorn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperature: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Spark dataframe for hourly weather\n",
    "whdDf = spark.read.csv('./darksky/weather_hourly_darksky.csv', inferSchema=True, header=True)\n",
    "\n",
    "whdDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- day: timestamp (nullable = true)\n",
      " |-- hh_0: double (nullable = true)\n",
      " |-- hh_1: double (nullable = true)\n",
      " |-- hh_2: double (nullable = true)\n",
      " |-- hh_3: double (nullable = true)\n",
      " |-- hh_4: double (nullable = true)\n",
      " |-- hh_5: double (nullable = true)\n",
      " |-- hh_6: double (nullable = true)\n",
      " |-- hh_7: double (nullable = true)\n",
      " |-- hh_8: double (nullable = true)\n",
      " |-- hh_9: double (nullable = true)\n",
      " |-- hh_10: double (nullable = true)\n",
      " |-- hh_11: double (nullable = true)\n",
      " |-- hh_12: double (nullable = true)\n",
      " |-- hh_13: double (nullable = true)\n",
      " |-- hh_14: double (nullable = true)\n",
      " |-- hh_15: double (nullable = true)\n",
      " |-- hh_16: double (nullable = true)\n",
      " |-- hh_17: double (nullable = true)\n",
      " |-- hh_18: double (nullable = true)\n",
      " |-- hh_19: double (nullable = true)\n",
      " |-- hh_20: double (nullable = true)\n",
      " |-- hh_21: double (nullable = true)\n",
      " |-- hh_22: double (nullable = true)\n",
      " |-- hh_23: double (nullable = true)\n",
      " |-- hh_24: double (nullable = true)\n",
      " |-- hh_25: double (nullable = true)\n",
      " |-- hh_26: double (nullable = true)\n",
      " |-- hh_27: double (nullable = true)\n",
      " |-- hh_28: double (nullable = true)\n",
      " |-- hh_29: double (nullable = true)\n",
      " |-- hh_30: double (nullable = true)\n",
      " |-- hh_31: double (nullable = true)\n",
      " |-- hh_32: double (nullable = true)\n",
      " |-- hh_33: double (nullable = true)\n",
      " |-- hh_34: double (nullable = true)\n",
      " |-- hh_35: double (nullable = true)\n",
      " |-- hh_36: double (nullable = true)\n",
      " |-- hh_37: double (nullable = true)\n",
      " |-- hh_38: double (nullable = true)\n",
      " |-- hh_39: double (nullable = true)\n",
      " |-- hh_40: double (nullable = true)\n",
      " |-- hh_41: double (nullable = true)\n",
      " |-- hh_42: double (nullable = true)\n",
      " |-- hh_43: double (nullable = true)\n",
      " |-- hh_44: double (nullable = true)\n",
      " |-- hh_45: double (nullable = true)\n",
      " |-- hh_46: double (nullable = true)\n",
      " |-- hh_47: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create Spark dataframe for hhblock\n",
    "hhblockDf = spark.read.csv('./darksky/hhblock_dataset/hhblock_dataset', inferSchema=True, header=True)\n",
    "\n",
    "hhblockDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-------------+-------+\n",
      "|    LCLid|stdorToU|  Acorn|Acorn_grouped|   file|\n",
      "+---------+--------+-------+-------------+-------+\n",
      "|MAC005492|     ToU| ACORN-|       ACORN-|block_0|\n",
      "|MAC001074|     ToU| ACORN-|       ACORN-|block_0|\n",
      "|MAC000002|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003613|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003597|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003579|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003566|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003557|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003553|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003482|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003463|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003449|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003428|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003423|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003422|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003400|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003394|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003388|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC003348|     Std|ACORN-A|     Affluent|block_0|\n",
      "|MAC000246|     Std|ACORN-A|     Affluent|block_0|\n",
      "+---------+--------+-------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "informationDf = spark.read.csv('./darksky/informations_households.csv', inferSchema=True, header=True)\n",
    "\n",
    "informationDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|      Bank holidays|                Type|\n",
      "+-------------------+--------------------+\n",
      "|2012-12-26 00:00:00|          Boxing Day|\n",
      "|2012-12-25 00:00:00|       Christmas Day|\n",
      "|2012-08-27 00:00:00| Summer bank holiday|\n",
      "|2012-05-06 00:00:00|Queen?s Diamond J...|\n",
      "|2012-04-06 00:00:00|Spring bank holid...|\n",
      "|2012-07-05 00:00:00|Early May bank ho...|\n",
      "|2012-09-04 00:00:00|       Easter Monday|\n",
      "|2012-06-04 00:00:00|         Good Friday|\n",
      "|2012-02-01 00:00:00|New Year?s Day (s...|\n",
      "|2013-12-26 00:00:00|          Boxing Day|\n",
      "|2013-12-25 00:00:00|       Christmas Day|\n",
      "|2013-08-26 00:00:00| Summer bank holiday|\n",
      "|2013-05-27 00:00:00| Spring bank holiday|\n",
      "|2013-06-05 00:00:00|Early May bank ho...|\n",
      "|2013-01-04 00:00:00|       Easter Monday|\n",
      "|2013-03-29 00:00:00|         Good Friday|\n",
      "|2013-01-01 00:00:00|      New Year?s Day|\n",
      "|2014-12-26 00:00:00|          Boxing Day|\n",
      "|2014-12-25 00:00:00|       Christmas Day|\n",
      "|2014-08-25 00:00:00| Summer bank holiday|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "holidaysDf = spark.read.csv('./darksky/uk_bank_holidays.csv', inferSchema=True, header=True)\n",
    "\n",
    "holidaysDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMaxTime: timestamp (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- temperatureMinTime: timestamp (nullable = true)\n",
      " |-- cloudCover: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperatureMinTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureHigh: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- apparentTemperatureHighTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLow: double (nullable = true)\n",
      " |-- apparentTemperatureMax: double (nullable = true)\n",
      " |-- uvIndex: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- sunsetTime: timestamp (nullable = true)\n",
      " |-- temperatureLow: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      " |-- temperatureHigh: double (nullable = true)\n",
      " |-- sunriseTime: timestamp (nullable = true)\n",
      " |-- temperatureHighTime: timestamp (nullable = true)\n",
      " |-- uvIndexTime: timestamp (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- temperatureLowTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureMin: double (nullable = true)\n",
      " |-- apparentTemperatureMaxTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLowTime: timestamp (nullable = true)\n",
      " |-- moonPhase: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wddDf = spark.read.csv('./darksky/weather_daily_darksky.csv', inferSchema=True, header=True)\n",
    "\n",
    "wddDf.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X acorn_details.csv                       X informations_households.csv\n",
    "X daily_dataset                           X uk_bank_holidays.csv\n",
    "/ darksky_parameters_documentation.html   X weather_daily_darksky.csv\n",
    "X halfhourly_dataset                      X weather_hourly_darksky.csv\n",
    "X hhblock_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:====================================================>  (97 + 5) / 102]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- tstp: timestamp (nullable = true)\n",
      " |-- energy(kWh/hh): string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "halfhourDf = spark.read.csv('./darksky/halfhourly_dataset/halfhourly_dataset', inferSchema=True, header=True)\n",
    "\n",
    "halfhourDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n",
      "23/05/20 19:22:19 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[temperatureMax: double, temperatureMaxTime: timestamp, windBearing: int, icon: string, dewPoint: double, temperatureMinTime: timestamp, cloudCover: double, windSpeed: double, pressure: double, apparentTemperatureMinTime: timestamp, apparentTemperatureHigh: double, precipType: string, visibility: double, humidity: double, apparentTemperatureHighTime: timestamp, apparentTemperatureLow: double, apparentTemperatureMax: double, uvIndex: double, time: timestamp, sunsetTime: timestamp, temperatureLow: double, temperatureMin: double, temperatureHigh: double, sunriseTime: timestamp, temperatureHighTime: timestamp, uvIndexTime: timestamp, summary: string, temperatureLowTime: timestamp, apparentTemperatureMin: double, apparentTemperatureMaxTime: timestamp, apparentTemperatureLowTime: timestamp, moonPhase: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache datasets\n",
    "dailyDf.cache()\n",
    "\n",
    "acorn.cache()\n",
    "\n",
    "halfhourDf.cache()\n",
    "\n",
    "hhblockDf.cache()\n",
    "\n",
    "informationDf.cache()\n",
    "\n",
    "holidaysDf.cache()\n",
    "\n",
    "whdDf.cache()\n",
    "\n",
    "wddDf.cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
